{
  "metadata": {
    "name": "Process Cricket Data",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n\r\n# Define the GCS bucket and paths\r\nraw_bucket \u003d \"cricket-raw\"\r\nprocessed_bucket \u003d \"cricket-processed\" # Make sure this bucket exists\r\n\r\n# We\u0027ll test with the T20 data first\r\ninput_path \u003d f\"gs://{raw_bucket}/t20_json/\"\r\noutput_path \u003d f\"gs://{processed_bucket}/test_output\"\r\n\r\nprint(f\"Input path set to: {input_path}\")\r\nprint(f\"Output path set to: {output_path}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\nfirst_file_rdd \u003d spark.sparkContext.wholeTextFiles(input_path).take(1)\n\n# The result is a list containing one item: (filepath, file_content).\n# We only need the content, which is the second element [1].\nfirst_file_content \u003d first_file_rdd[0][1]\n\n# Step 2: Create a new RDD containing just the text of that single file\n# and then create a DataFrame from it.\nsingle_file_df \u003d spark.read.option(\"multiLine\", \"true\").json(\n    spark.sparkContext.parallelize([first_file_content])\n)\n\n# Step 3: Print the schema that Spark infers from just that one file.\nprint(\"Schema inferred by Spark from one file:\")\nsingle_file_df.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n\r\nfrom pyspark.sql.types import StructType, StructField, StringType, ArrayType, LongType\r\n\r\n# 1. Define the full schema for the data in GCS\r\ndefined_schema \u003d StructType([\r\n    StructField(\"city\", StringType(), True),\r\n    StructField(\"dates\", ArrayType(StringType()), True),\r\n    StructField(\"event\", StructType([\r\n        StructField(\"name\", StringType(), True),\r\n        StructField(\"match_number\", LongType(), True)\r\n    ]), True),\r\n    StructField(\"gender\", StringType(), True),\r\n    StructField(\"match_type\", StringType(), True),\r\n    StructField(\"outcome\", StructType([\r\n        StructField(\"winner\", StringType(), True),\r\n        StructField(\"by\", StructType([\r\n            StructField(\"runs\", LongType(), True),\r\n            StructField(\"wickets\", LongType(), True)\r\n        ]), True)\r\n    ]), True),\r\n    StructField(\"overs\", StringType(), True),\r\n    StructField(\"player_of_match\", ArrayType(StringType()), True),\r\n    StructField(\"teams\", ArrayType(StringType()), True),\r\n    StructField(\"toss\", StructType([\r\n        StructField(\"decision\", StringType(), True),\r\n        StructField(\"winner\", StringType(), True)\r\n    ]), True),\r\n    StructField(\"venue\", StringType(), True)\r\n])\r\n\r\n# 2. Read the data using the schema\r\nraw_df \u003d spark.read.schema(defined_schema).option(\"multiLine\", \"true\").json(input_path)\r\nraw_df.cache()\r\n\r\n# 3. Verify\r\nprint(\"Successfully loaded data with the full schema.\")\r\nraw_df.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n\r\nfrom pyspark.sql.functions import col\r\n\r\n# Select and flatten all the specified fields\r\nfinal_df \u003d raw_df.select(\r\n    col(\"city\"),\r\n    col(\"dates\")[0].alias(\"match_date\"),\r\n    col(\"event.name\").alias(\"event_name\"),\r\n    col(\"gender\"),\r\n    col(\"match_type\").alias(\"match_type_detail\"),\r\n    col(\"outcome.winner\").alias(\"winner\"),\r\n    col(\"outcome.by.wickets\").alias(\"win_by_wickets\"),\r\n    col(\"outcome.by.runs\").alias(\"win_by_runs\"),\r\n    col(\"overs\"),\r\n    col(\"player_of_match\")[0].alias(\"player_of_match\"),\r\n    col(\"teams\")[0].alias(\"team_1\"),\r\n    col(\"teams\")[1].alias(\"team_2\"),\r\n    col(\"toss.decision\").alias(\"toss_decision\"),\r\n    col(\"toss.winner\").alias(\"toss_winner\"),\r\n    col(\"venue\")\r\n)\r\n\r\n# Create a temporary table for SQL queries\r\nfinal_df.createOrReplaceTempView(\"cricket_view_final\")\r\n\r\n# Show the comprehensive, flattened result\r\nprint(\"Final transformed data sample:\")\r\nfinal_df.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\r\n\r\n# Add the \u0027year\u0027 and \u0027match_type\u0027 columns for partitioning\r\npartitioned_df \u003d final_df.withColumn(\"year\", get_year(col(\"match_date\"))) \\\r\n                         .withColumn(\"match_type\", lit(\"t20\"))\r\n\r\n# Write the final DataFrame to the GCS output path in Parquet format\r\nprint(f\"Writing partitioned data to: {output_path}\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nprint(f\"partitioned_df columns: {partitioned_df.columns}\")\nprint(f\"partitioned_df count: {partitioned_df.count()}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n# Write the final DataFrame to the GCS output path\nprint(f\"Writing partitioned data to: {output_path}\")\npartitioned_df.write.partitionBy(\"match_type\", \"year\").mode(\"overwrite\").parquet(output_path)\n\nprint(\"\\nWrite complete!\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}